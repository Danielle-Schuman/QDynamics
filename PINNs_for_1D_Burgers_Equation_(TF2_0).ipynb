{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Mj-RBCn8MZ"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al80f-lPoJjh"
   },
   "source": [
    "## Getting the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "r3iHOMsdnNiq",
    "outputId": "48316653-e273-470c-daa6-f04891dde84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klone nach 'PINNs' ...\n",
      "remote: Enumerating objects: 741, done.\u001b[K\n",
      "remote: Total 741 (delta 0), reused 0 (delta 0), pack-reused 741\u001b[K\n",
      "Empfange Objekte: 100% (741/741), 474.47 MiB | 10.68 MiB/s, Fertig.\n",
      "Löse Unterschiede auf: 100% (66/66), Fertig.\n",
      "Aktualisiere Dateien: 100% (561/561), Fertig.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maziarraissi/PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtoc_dXgoOZq"
   },
   "source": [
    "## Setting up modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNMtDjXkFHaN"
   },
   "source": [
    "TeX packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TaZCKcDsEVRP",
    "outputId": "be5e2e31-f422-4c58-a821-0d7f7a38af9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -qq install texlive-fonts-recommended texlive-fonts-extra dvipng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otc4Ap7qFMlf"
   },
   "source": [
    "Pip modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "collapsed": true,
    "id": "srpq4aQNoQ1E",
    "outputId": "78582ecd-9664-4784-ab57-ef8276779e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyDOE\n",
      "  Using cached pyDOE-0.3.8-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.4.1)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (46.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.19.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.18.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (46.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (46.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.19.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Installing collected packages: tensorflow, pyDOE\n",
      "Successfully installed pyDOE-0.3.8 tensorflow-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pyDOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksKujMvUFRNW"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZEDn2fqlqctT",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODGREPvZpqUz"
   },
   "source": [
    "## Utilities: Data preparation, logger class and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FgPvqJiYFnYG",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
    "repoPath = os.path.join(\".\", \"PINNs\")\n",
    "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
    "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
    "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
    "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
    "\n",
    "sys.path.insert(0, utilsPath)\n",
    "from plotting import newfig, savefig\n",
    "\n",
    "# prepare data\n",
    "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
    "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
    "    data = scipy.io.loadmat(path)\n",
    "\n",
    "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
    "    t = data['t'].flatten()[:,None] # T x 1\n",
    "    x = data['x'].flatten()[:,None] # N x 1\n",
    "\n",
    "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
    "    Exact_u = np.real(data['usol']).T # T x N\n",
    "\n",
    "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
    "      dt = t[idx_t_1] - t[idx_t_0]\n",
    "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
    "      x_0 = x[idx_x,:]\n",
    "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
    "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "        \n",
    "      # Boudanry data\n",
    "      x_1 = np.vstack((lb, ub))\n",
    "      \n",
    "      # Test data\n",
    "      x_star = x\n",
    "      u_star = Exact_u[idx_t_1,:]\n",
    "\n",
    "      # Load IRK weights\n",
    "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
    "      IRK_times = tmp[q**2+q:]\n",
    "\n",
    "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
    "\n",
    "    # Meshing x and t in 2D (256,100)\n",
    "    X, T = np.meshgrid(x,t)\n",
    "\n",
    "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "    # Preparing the testing u_star\n",
    "    u_star = Exact_u.flatten()[:,None]\n",
    "                \n",
    "    # Noiseless data TODO: add support for noisy data    \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx,:]\n",
    "    u_train = u_star[idx,:]\n",
    "\n",
    "    if N_0 != None and N_1 != None:\n",
    "      Exact_u = Exact_u.T\n",
    "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
    "      x_0 = x[idx_x,:]\n",
    "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
    "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "          \n",
    "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
    "      x_1 = x[idx_x,:]\n",
    "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
    "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
    "      \n",
    "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
    "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
    "\n",
    "      # Load IRK weights\n",
    "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
    "      IRK_alpha = weights[0:-1,:]\n",
    "      IRK_beta = weights[-1:,:] \n",
    "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
    "\n",
    "    if N_f == None:\n",
    "      lb = X_star.min(axis=0)\n",
    "      ub = X_star.max(axis=0) \n",
    "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "    lb = X_star.min(axis=0)\n",
    "    ub = X_star.max(axis=0) \n",
    "    # Getting the initial conditions (t=0)\n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    uu1 = Exact_u[0:1,:].T\n",
    "    # Getting the lowest boundary conditions (x=-1) \n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    uu2 = Exact_u[:,0:1]\n",
    "    # Getting the highest boundary conditions (x=1) \n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    uu3 = Exact_u[:,-1:]\n",
    "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
    "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "    # Generating the x and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
    "    X_u_train = X_u_train[idx,:]\n",
    "    # Getting the corresponding u_train\n",
    "    u_train = u_train [idx,:]\n",
    "\n",
    "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
    "\n",
    "# define logger class\n",
    "class Logger(object):\n",
    "  def __init__(self, frequency=10):\n",
    "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "    self.start_time = time.time()\n",
    "    self.frequency = frequency\n",
    "\n",
    "  def __get_elapsed(self):\n",
    "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "  def __get_error_u(self):\n",
    "    return self.error_fn()\n",
    "\n",
    "  def set_error_fn(self, error_fn):\n",
    "    self.error_fn = error_fn\n",
    "  \n",
    "  def log_train_start(self, model):\n",
    "    print(\"\\nTraining started\")\n",
    "    print(\"================\")\n",
    "    self.model = model\n",
    "    print(self.model.summary())\n",
    "\n",
    "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "    if epoch % self.frequency == 0:\n",
    "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "  def log_train_opt(self, name):\n",
    "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "    print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "  def log_train_end(self, epoch, custom=\"\"):\n",
    "    print(\"==================\")\n",
    "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "# for plotting\n",
    "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
    "\n",
    "  # Interpolating the results on the whole (x,t) domain.\n",
    "  # griddata(points, values, points at which to interpolate, method)\n",
    "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
    "\n",
    "  # Creating the figures\n",
    "  fig, ax = newfig(1.0, 1.1)\n",
    "  ax.axis('off')\n",
    "\n",
    "  ####### Row 0: u(t,x) ##################    \n",
    "  gs0 = gridspec.GridSpec(1, 2)\n",
    "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "  ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "  divider = make_axes_locatable(ax)\n",
    "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "  fig.colorbar(h, cax=cax)\n",
    "\n",
    "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "  ax.set_xlabel('$t$')\n",
    "  ax.set_ylabel('$x$')\n",
    "  ax.legend(frameon=False, loc = 'best')\n",
    "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "  ####### Row 1: u(t,x) slices ##################    \n",
    "  gs1 = gridspec.GridSpec(1, 3)\n",
    "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 0])\n",
    "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')    \n",
    "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 1])\n",
    "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 2])\n",
    "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])    \n",
    "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  if file != None:\n",
    "    fig.savefig(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Azv7DZp0M-"
   },
   "source": [
    "## Define custom lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OjvMe1Avpvh9",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "  global global_time_list, global_last_time\n",
    "  global_time_list = []\n",
    "  global_last_time = time.perf_counter()\n",
    "  \n",
    "def record_time():\n",
    "  global global_last_time, global_time_list\n",
    "  new_time = time.perf_counter()\n",
    "  global_time_list.append(new_time - global_last_time)\n",
    "  global_last_time = time.perf_counter()\n",
    "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "  \"\"\"Returns last interval records in millis.\"\"\"\n",
    "  global global_last_time, global_time_list\n",
    "  if global_time_list:\n",
    "    return 1000 * global_time_list[-1]\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def dot(a, b):\n",
    "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "  return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "  print(s)\n",
    "\n",
    "final_loss = None\n",
    "times = []\n",
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "  \"\"\"\n",
    "\n",
    "  if config.maxIter == 0:\n",
    "    return\n",
    "\n",
    "  global final_loss, times\n",
    "  \n",
    "  maxIter = config.maxIter\n",
    "  maxEval = config.maxEval or maxIter*1.25\n",
    "  tolFun = config.tolFun or 1e-5\n",
    "  tolX = config.tolX or 1e-19\n",
    "  nCorrection = config.nCorrection or 100\n",
    "  lineSearch = config.lineSearch\n",
    "  lineSearchOpts = config.lineSearchOptions\n",
    "  learningRate = config.learningRate or 1\n",
    "  isverbose = config.verbose or False\n",
    "\n",
    "  # verbose function\n",
    "  if isverbose:\n",
    "    verbose = verbose_func\n",
    "  else:\n",
    "    verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "  f, g = opfunc(x)\n",
    "\n",
    "  f_hist = [f]\n",
    "  currentFuncEval = 1\n",
    "  state.funcEval = state.funcEval + 1\n",
    "  p = g.shape[0]\n",
    "\n",
    "  # check optimality of initial point\n",
    "  tmp1 = tf.abs(g)\n",
    "  if tf.reduce_sum(tmp1) <= tolFun:\n",
    "    verbose(\"optimality condition below tolFun\")\n",
    "    return x, f_hist\n",
    "\n",
    "  # optimize for a max of maxIter iterations\n",
    "  nIter = 0\n",
    "  times = []\n",
    "  while nIter < maxIter:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # keep track of nb of iterations\n",
    "    nIter = nIter + 1\n",
    "    state.nIter = state.nIter + 1\n",
    "\n",
    "    ############################################################\n",
    "    ## compute gradient descent direction\n",
    "    ############################################################\n",
    "    if state.nIter == 1:\n",
    "      d = -g\n",
    "      old_dirs = []\n",
    "      old_stps = []\n",
    "      Hdiag = 1\n",
    "    else:\n",
    "      # do lbfgs update (update memory)\n",
    "      y = g - g_old\n",
    "      s = d*t\n",
    "      ys = dot(y, s)\n",
    "      \n",
    "      if ys > 1e-10:\n",
    "        # updating memory\n",
    "        if len(old_dirs) == nCorrection:\n",
    "          # shift history by one (limited-memory)\n",
    "          del old_dirs[0]\n",
    "          del old_stps[0]\n",
    "\n",
    "        # store new direction/step\n",
    "        old_dirs.append(s)\n",
    "        old_stps.append(y)\n",
    "\n",
    "        # update scale of initial Hessian approximation\n",
    "        Hdiag = ys/dot(y, y)\n",
    "\n",
    "      # compute the approximate (L-BFGS) inverse Hessian \n",
    "      # multiplied by the gradient\n",
    "      k = len(old_dirs)\n",
    "\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      ro = [0]*nCorrection\n",
    "      for i in range(k):\n",
    "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "        \n",
    "\n",
    "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      al = [0]*nCorrection\n",
    "\n",
    "      q = -g\n",
    "      for i in range(k-1, -1, -1):\n",
    "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "        q = q - al[i]*old_stps[i]\n",
    "\n",
    "      # multiply by initial Hessian\n",
    "      r = q*Hdiag\n",
    "      for i in range(k):\n",
    "        be_i = dot(old_stps[i], r) * ro[i]\n",
    "        r += (al[i]-be_i)*old_dirs[i]\n",
    "        \n",
    "      d = r\n",
    "      # final direction is in r/d (same object)\n",
    "\n",
    "    g_old = g\n",
    "    f_old = f\n",
    "    \n",
    "    ############################################################\n",
    "    ## compute step length\n",
    "    ############################################################\n",
    "    # directional derivative\n",
    "    gtd = dot(g, d)\n",
    "\n",
    "    # check that progress can be made along that direction\n",
    "    if gtd > -tolX:\n",
    "      verbose(\"Can not make progress along direction.\")\n",
    "      break\n",
    "\n",
    "    # reset initial guess for step size\n",
    "    if state.nIter == 1:\n",
    "      tmp1 = tf.abs(g)\n",
    "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "    else:\n",
    "      t = learningRate\n",
    "\n",
    "\n",
    "    # optional line search: user function\n",
    "    lsFuncEval = 0\n",
    "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "      # perform line search, using user function\n",
    "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "      f_hist.append(f)\n",
    "    else:\n",
    "      # no line search, simply move with fixed-step\n",
    "      x += t*d\n",
    "      \n",
    "      if nIter != maxIter:\n",
    "        # re-evaluate function only if not in last iteration\n",
    "        # the reason we do this: in a stochastic setting,\n",
    "        # no use to re-evaluate that function here\n",
    "        f, g = opfunc(x)\n",
    "        lsFuncEval = 1\n",
    "        f_hist.append(f)\n",
    "\n",
    "\n",
    "    # update func eval\n",
    "    currentFuncEval = currentFuncEval + lsFuncEval\n",
    "    state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "    ############################################################\n",
    "    ## check conditions\n",
    "    ############################################################\n",
    "    if nIter == maxIter:\n",
    "      break\n",
    "\n",
    "    if currentFuncEval >= maxEval:\n",
    "      # max nb of function evals\n",
    "      verbose('max nb of function evals')\n",
    "      break\n",
    "\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <=tolFun:\n",
    "      # check optimality\n",
    "      verbose('optimality condition below tolFun')\n",
    "      break\n",
    "    \n",
    "    tmp1 = tf.abs(d*t)\n",
    "    if tf.reduce_sum(tmp1) <= tolX:\n",
    "      # step size below tolX\n",
    "      verbose('step size below tolX')\n",
    "      break\n",
    "\n",
    "    if tf.abs(f-f_old) < tolX:\n",
    "      # function value changing less than tolX\n",
    "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "      break\n",
    "\n",
    "    if do_verbose:\n",
    "      log_fn(nIter, f.numpy(), True)\n",
    "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "      record_time()\n",
    "      times.append(last_time())\n",
    "\n",
    "    if nIter == maxIter - 1:\n",
    "      final_loss = f.numpy()\n",
    "\n",
    "\n",
    "  # save state\n",
    "  state.old_dirs = old_dirs\n",
    "  state.old_stps = old_stps\n",
    "  state.Hdiag = Hdiag\n",
    "  state.g_old = g_old\n",
    "  state.f_old = f_old\n",
    "  state.t = t\n",
    "  state.d = d\n",
    "\n",
    "  return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "  pass\n",
    "\n",
    "class Struct(dummy):\n",
    "  def __getattribute__(self, key):\n",
    "    if key == '__dict__':\n",
    "      return super(dummy, self).__getattribute__('__dict__')\n",
    "    return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOT-E8C4oAJN"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qrt3ECzcLHp"
   },
   "source": [
    "Burger' Equation:\n",
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8CHqrpafela"
   },
   "source": [
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9Ko6L87J2v_"
   },
   "source": [
    "### Hyperparameters for the classical PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jwWhiecUqbAo",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "#layers = [2, 20, 2, 3, 2, 20, 1]\n",
    "\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 500 #1000-2000 way more accurate but this is quicker ...\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkimJNtepkKi"
   },
   "source": [
    "## PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KVm9UCvvlyY_"
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "    # Descriptive Sequential Keras model [2, 20, …, 20, 1]\n",
    "    self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]: # add dense layers\n",
    "      self.u_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.nu = nu\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Separating the collocation coordinates\n",
    "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "    \n",
    "  # Defining custom loss\n",
    "  def __loss(self, u, u_pred):\n",
    "    f_pred = self.f_model()\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  # The actual PINN\n",
    "  def f_model(self):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(self.x_f)\n",
    "      tape.watch(self.t_f)\n",
    "      # Packing together the inputs\n",
    "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      u_x = tape.gradient(u, self.x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, self.x_f)\n",
    "    u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    nu = self.get_params(numpy=True)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.nu\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.u_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.f_model()\n",
    "    return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FzMd65dpoHo"
   },
   "source": [
    "## Training and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SEKkpHvApf46",
    "lines_to_next_cell": 2,
    "outputId": "8a6c27ef-8be5-431b-ab60-c4ddd6edb429",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n",
      "Eager execution: True\n",
      "GPU-accerelated: False\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,021\n",
      "Trainable params: 3,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf_epoch =      0  elapsed = 00:00  loss = 5.9277e-01  error = 9.2492e-01  \n",
      "tf_epoch =     10  elapsed = 00:04  loss = 4.4961e-01  error = 1.0874e+00  \n",
      "tf_epoch =     20  elapsed = 00:07  loss = 3.5029e-01  error = 9.5520e-01  \n",
      "tf_epoch =     30  elapsed = 00:09  loss = 2.7380e-01  error = 7.9232e-01  \n",
      "tf_epoch =     40  elapsed = 00:11  loss = 2.4649e-01  error = 8.0986e-01  \n",
      "tf_epoch =     50  elapsed = 00:13  loss = 2.6655e-01  error = 9.9817e-01  \n",
      "tf_epoch =     60  elapsed = 00:16  loss = 1.9417e-01  error = 7.3750e-01  \n",
      "tf_epoch =     70  elapsed = 00:18  loss = 2.0500e-01  error = 6.5448e-01  \n",
      "tf_epoch =     80  elapsed = 00:20  loss = 2.0559e-01  error = 6.6207e-01  \n",
      "tf_epoch =     90  elapsed = 00:22  loss = 1.8294e-01  error = 6.9237e-01  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:27  loss = 1.3979e-01  error = 7.0022e-01  \n",
      "nt_epoch =     20  elapsed = 00:30  loss = 1.1076e-01  error = 6.0340e-01  \n",
      "nt_epoch =     30  elapsed = 00:33  loss = 9.5944e-02  error = 6.0172e-01  \n",
      "nt_epoch =     40  elapsed = 00:36  loss = 8.9687e-02  error = 6.0731e-01  \n",
      "nt_epoch =     50  elapsed = 00:40  loss = 8.2611e-02  error = 6.4959e-01  \n"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
    "def error():\n",
    "  u_pred, _ = pinn.predict(X_star)\n",
    "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "Wf1QXaK5PlUh",
    "lines_to_next_cell": 0,
    "outputId": "7c9c2826-ba07-433a-ab68-73cd0f7ea2e0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.543x264.146 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "  Exact_u, X, T, x, t, file = \"plot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968487"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-3.1513e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxls pennylane try, does not work unfortunately\n",
    "import pennylane as qml\n",
    "\n",
    "\n",
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits) # \"lightning.qubit\" fast, C++ device\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n",
    "\n",
    "weight_shapes = {\"weights\": (3, n_qubits, 3)}\n",
    "\n",
    "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# TFQ / Cirq code for quantum layer: 1 node, input-vector-length = 4 (TODO: make 16)\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "from cirq import H, X, cphase, CNOT, Z, T\n",
    "from cirq.circuits import InsertStrategy\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "\n",
    "# adapted from https://github.com/ghellstern/QuantumNN/blob/master/Multi-QBit-Classifier%20TF%20NN-Encoding_Github.ipynb\n",
    "class SplitBackpropQ(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, upstream_symbols, managed_symbols, managed_init_vals,\n",
    "                 operators):\n",
    "        \"\"\"Create a layer that splits backprop between several variables.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            upstream_symbols: Python iterable of symbols to bakcprop\n",
    "                through this layer.\n",
    "            managed_symbols: Python iterable of symbols to backprop\n",
    "                into variables managed by this layer.\n",
    "            managed_init_vals: Python iterable of initial values\n",
    "                for managed_symbols.\n",
    "            operators: Python iterable of operators to use for expectation.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(SplitBackpropQ)\n",
    "        self.all_symbols = upstream_symbols + managed_symbols\n",
    "        self.upstream_symbols = upstream_symbols\n",
    "        self.managed_symbols = managed_symbols\n",
    "        self.managed_init = managed_init_vals\n",
    "        self.ops = operators\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.managed_weights = self.add_weight(\n",
    "            shape=(1, len(self.managed_symbols)),\n",
    "            initializer=tf.constant_initializer(self.managed_init))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs are: circuit tensor, upstream values\n",
    "        upstream_shape = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_weights = tf.tile(self.managed_weights, [upstream_shape, 1])\n",
    "        joined_params = tf.concat([inputs[1], tiled_up_weights], 1)\n",
    "        return tfq.layers.Expectation()(inputs[0],\n",
    "                                        operators=measurement,\n",
    "                                        symbol_names=self.all_symbols,\n",
    "                                        symbol_values=joined_params)\n",
    "\n",
    "\n",
    "# TODO: Normalize weights and inputs to be between [0, pi / 2]\n",
    "# TODO: replace CPhase-Gate with Multi-controlled Phase gate and use 4 qubits, Input-Size 16 instead\n",
    "# 2 regular qubits, 1 ancilla\n",
    "number_qubits = 3\n",
    "number_regular_qubits = number_qubits - 1\n",
    "\n",
    "# specify parameters to set later with NN inputs and weights using Keras + TFQ\n",
    "regular_qubits = [cirq.GridQubit(i, 0) for i in range(number_regular_qubits)]\n",
    "ancilla = cirq.GridQubit(number_regular_qubits, 0)\n",
    "control_params = sympy.symbols('i0, i1, i2, i3')\n",
    "control_params1 = sympy.symbols('w0, w1, w2, w3')\n",
    "\n",
    "# specify cirq circuit\n",
    "qc = cirq.Circuit()\n",
    "size = len(regular_qubits) ** 2\n",
    "# subtract first input from other inputs to save gates\n",
    "#inputs = []\n",
    "#for i in range(1, size):\n",
    "    #inputs.append(control_params[i] - control_params[0])\n",
    "# do the same for weights\n",
    "#weights = []\n",
    "#for i in range(1, size):\n",
    "    #weights.append(control_params1[i] - control_params1[0])\n",
    "# apply Hadamard gate to all regular qubits to create a superposition\n",
    "qc.append(H.on_each(*regular_qubits))\n",
    "# loop over all inputs in inputvector to encode them to the right base states using phase-shifts\n",
    "for index in range(size):\n",
    "    insert_list = []\n",
    "    # index as binary number\n",
    "    binary = '{0:02b}'.format(index)\n",
    "    # get qubit at digit in binary state (positions of qubits : q0, q1, q2, q3) (figuratively, not actually, we are in superposition after all)\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    # this_phase_gate = MCPhaseGate(value, 3, label=\"this_phase_gate\")\n",
    "    # qc.this_phase_gate(0, 1, 2, 3)\n",
    "    # perform controlled phase shift (for more qubits probably possible using ControlledGate() and MatrixGate()\n",
    "    insert_list.append(cphase(control_params[index])(*regular_qubits))\n",
    "    # \"undo\" the NOT-gates to get back to previous states = apply another not\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    qc.append(insert_list, strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# loop over weights\n",
    "for w in range(size):\n",
    "    insert_list = []\n",
    "    # index as binary number\n",
    "    binary = '{0:02b}'.format(w)\n",
    "    # get qubit at digit in binary state (positions of qubits : q0, q1, q2, q3) (figuratively, not actually, we are in superposition after all)\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    # this_phase_gate = MCPhaseGate(value, 3, label=\"this_phase_gate\")\n",
    "    # qc.this_phase_gate(0, 1, 2, 3)\n",
    "    # perform conjugate transpose controlled phase shift\n",
    "    insert_list.append(cphase((-1) * control_params1[w])(*regular_qubits))\n",
    "    # \"undo\" the NOT-gates to get back to previous states = apply another not\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    qc.append(insert_list, strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# apply Hadamard gate to all regular qubits\n",
    "qc.append(H.on_each(*regular_qubits), strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# apply X gate to all regular qubits\n",
    "qc.append(X.on_each(*regular_qubits), strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# collect combined state from all regular qubits with ancilla qubit using Toffoli-Gate\n",
    "# Toffoli-Gate does not work in TFQ -> implement decomposition (compare https://en.wikipedia.org/wiki/Toffoli_gate#/media/File:Qcircuit_ToffolifromCNOT.svg)\n",
    "qc.append([H(ancilla), CNOT(regular_qubits[1], ancilla), cirq.inverse(T(ancilla)), CNOT(regular_qubits[0], ancilla), T(ancilla), CNOT(regular_qubits[1], ancilla), cirq.inverse(T(ancilla)), CNOT(regular_qubits[0], ancilla), T(ancilla), T(regular_qubits[1]), H(ancilla), CNOT(regular_qubits[0], regular_qubits[1]), cirq.inverse(T(regular_qubits[1])), T(regular_qubits[0]), CNOT(regular_qubits[0], regular_qubits[1])], strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# draw circuit\n",
    "SVGCircuit(qc)\n",
    "# end circuit\n",
    "\n",
    "# values to initialize the weights (?)\n",
    "np.random.seed(seed=69)\n",
    "int_values = np.random.rand((len(control_params1)))*np.pi\n",
    "\n",
    "measurement = [Z(ancilla)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define QPINN class\n",
    "class QPhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu, control_params, control_params1, int_values, measurement):\n",
    "    # Keras model\n",
    "    \"\"\"self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\"\"\"\n",
    "    # one additional input layer (unused) is needed for the quantum circuit as input for the quantum layer\n",
    "    self.unused = tf.keras.Input(shape=(), dtype=tf.dtypes.string, name=\"unused\")\n",
    "    self.inputs = tf.keras.layers.Input(shape=(layers[0],))\n",
    "    \n",
    "    # define custom layer for lambda layer\n",
    "    def custom_layer(X):\n",
    "        return 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    self.outputs = tf.keras.layers.Lambda(custom_layer, name=\"lambda_layer\")(self.inputs)\n",
    "    \n",
    "    #self.outputs = tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0)(self.inputs)\n",
    "    for width in layers[1:]:\n",
    "        self.outputs = tf.keras.layers.Dense(width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal')(self.outputs)\n",
    "    #quantum layer\n",
    "    self.outputs = SplitBackpropQ(control_params, control_params1, int_values, measurement)([self.unused, self.outputs])\n",
    "    #generate model\n",
    "    self.u_model = tf.keras.Model(inputs=[self.unused, self.inputs], outputs=self.outputs)\n",
    "    \n",
    "    #inputs = tf.keras.Input(shape=(3,))\n",
    "    #x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
    "    #outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
    "    #model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \"\"\"\n",
    "    unused = tf.keras.Input(shape=(), dtype=tf.dtypes.string)\n",
    "    inputlayer = keras.Input(shape=(32,), name=\"inputlayer\")\n",
    "    x = layers.Dense(32, activation=\"relu\")(inputlayer)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(4, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    expectation = SplitBackpropQ(control_params, control_params1, int_values, measurement)([unused, x])\n",
    "    #expectation = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=[unused, inputlayer], outputs=expectation)\"\"\"\n",
    "    \n",
    "    \"\"\"for width in layers[1:]:\n",
    "        if width == 3:\n",
    "            self.u_model.add(qlayer)\n",
    "        else:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\"\"\"\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.nu = nu\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Separating the collocation coordinates\n",
    "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "    \n",
    "  # Defining custom loss\n",
    "  def __loss(self, u, u_pred):\n",
    "    f_pred = self.f_model()\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  # The actual PINN\n",
    "  def f_model(self):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(self.x_f)\n",
    "      tape.watch(self.t_f)\n",
    "      # Packing together the inputs\n",
    "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      u_x = tape.gradient(u, self.x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, self.x_f)\n",
    "    u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    nu = self.get_params(numpy=True)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.nu\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.u_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    \"\"\"lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\"\"\"\n",
    "\n",
    "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.f_model()\n",
    "    return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters quantum PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 1000\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.3,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 500 # 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n",
      "Eager execution: True\n",
      "GPU-accerelated: False\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"functional_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_layer (Lambda)           (None, 2)            0           input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 20)           60          lambda_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 20)           420         dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, 20)           420         dense_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, 20)           420         dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_126 (Dense)               (None, 20)           420         dense_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_127 (Dense)               (None, 20)           420         dense_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_128 (Dense)               (None, 20)           420         dense_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_129 (Dense)               (None, 20)           420         dense_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "unused (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_130 (Dense)               (None, 1)            21          dense_129[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "split_backprop_q_3 (SplitBackpr (None, 1)            4           unused[0][0]                     \n",
      "                                                                 dense_130[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,025\n",
      "Trainable params: 3,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Cast float to string is not supported [Op:Cast]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4b7363320dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mqpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-effd4268b0fe>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, tf_epochs, nt_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;31m# Optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m       \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrap_training_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-effd4268b0fe>\u001b[0m in \u001b[0;36m__grad\u001b[0;34m(self, X, u)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrap_training_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \"\"\"\n\u001b[1;32m    385\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 386\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mtensor_usage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_usage_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conform_to_reference_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m       \u001b[0mx_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtensor_usage_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_conform_to_reference_input\u001b[0;34m(self, tensor, ref_input)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;31m# Dtype casting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0;31m# Dtype casting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    920\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   1856\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Cast float to string is not supported [Op:Cast]"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "nu=0.01/np.pi\n",
    "qpinn = QPhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu, control_params, control_params1, int_values, measurement)\n",
    "def error():\n",
    "  u_pred, _ = qpinn.predict(X_star)\n",
    "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "qpinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = qpinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.543x264.146 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "  Exact_u, X, T, x, t, file = \"qplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "A1Mj-RBCn8MZ",
    "GkimJNtepkKi",
    "dOPzdkKsJzA4",
    "OTxvp1nJGDeb",
    "QGd5zVtoxAqt",
    "bXtJ5GiaxAqw",
    "fGrMDRc3w1ex",
    "rRGW4IW0w1e0"
   ],
   "name": "PINNs for 1D Burgers Equation (TF2.0).ipynb",
   "provenance": []
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
