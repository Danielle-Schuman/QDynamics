{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Mj-RBCn8MZ"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al80f-lPoJjh"
   },
   "source": [
    "## Getting the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3iHOMsdnNiq",
    "outputId": "db06b389-9db6-43ca-8a2d-d66aaf14fdff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klone nach 'PINNs' ...\n",
      "remote: Enumerating objects: 741, done.\u001b[K\n",
      "remote: Total 741 (delta 0), reused 0 (delta 0), pack-reused 741\u001b[K\n",
      "Empfange Objekte: 100% (741/741), 474.47 MiB | 10.66 MiB/s, Fertig.\n",
      "Löse Unterschiede auf: 100% (66/66), Fertig.\n",
      "Aktualisiere Dateien: 100% (561/561), Fertig.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maziarraissi/PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtoc_dXgoOZq"
   },
   "source": [
    "## Setting up modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNMtDjXkFHaN"
   },
   "source": [
    "TeX packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaZCKcDsEVRP",
    "outputId": "abb92446-3fce-4be6-cc89-dacdaa527d9b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -qq install texlive-fonts-recommended texlive-fonts-extra dvipng cm-super"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otc4Ap7qFMlf"
   },
   "source": [
    "Pip modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "srpq4aQNoQ1E",
    "outputId": "dc5855bf-5251-433a-b45d-82e812199e8e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyDOE in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.3.8)\n",
      "Requirement already satisfied: tensorflow_quantum in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: tensorflow_probability==0.11.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.11.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.32.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.11.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==2.3.1) (3.12.2)\n",
      "Requirement already satisfied: dm-tree in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow_probability==0.11.1) (0.1.5)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow_probability==0.11.1) (4.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow_probability==0.11.1) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow==2.3.1) (46.2.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.19.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyDOE) (1.4.1)\n",
      "Requirement already satisfied: cirq==0.9.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow_quantum) (0.9.1)\n",
      "Requirement already satisfied: sympy==1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow_quantum) (1.5)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (0.25.3)\n",
      "Requirement already satisfied: freezegun~=0.3.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (0.3.15)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (1.21.0)\n",
      "Requirement already satisfied: networkx~=2.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (2.4)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (3.7.4.2)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (3.1.2)\n",
      "Requirement already satisfied: sortedcontainers~=2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cirq==0.9.1->tensorflow_quantum) (2.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sympy==1.5->tensorflow_quantum) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil!=2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from freezegun~=0.3.15->cirq==0.9.1->tensorflow_quantum) (2.8.1)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow_quantum) (2019.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow_quantum) (1.52.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow_quantum) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow_quantum) (2.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow_quantum) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1 pyDOE tensorflow_quantum tensorflow_probability==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb7pzZkgYz2j"
   },
   "source": [
    "If you are asked to restart the runtime, please do so in order to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksKujMvUFRNW"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZEDn2fqlqctT",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODGREPvZpqUz"
   },
   "source": [
    "## Utilities: Data preparation, logger class and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FgPvqJiYFnYG",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
    "repoPath = os.path.join(\".\", \"PINNs\")\n",
    "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
    "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
    "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
    "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
    "\n",
    "sys.path.insert(0, utilsPath)\n",
    "from plotting import newfig, savefig\n",
    "\n",
    "# prepare data\n",
    "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
    "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
    "    data = scipy.io.loadmat(path)\n",
    "\n",
    "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
    "    t = data['t'].flatten()[:,None] # T x 1\n",
    "    x = data['x'].flatten()[:,None] # N x 1\n",
    "\n",
    "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
    "    Exact_u = np.real(data['usol']).T # T x N\n",
    "\n",
    "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
    "      dt = t[idx_t_1] - t[idx_t_0]\n",
    "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
    "      x_0 = x[idx_x,:]\n",
    "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
    "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "        \n",
    "      # Boudanry data\n",
    "      x_1 = np.vstack((lb, ub))\n",
    "      \n",
    "      # Test data\n",
    "      x_star = x\n",
    "      u_star = Exact_u[idx_t_1,:]\n",
    "\n",
    "      # Load IRK weights\n",
    "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
    "      IRK_times = tmp[q**2+q:]\n",
    "\n",
    "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
    "\n",
    "    # Meshing x and t in 2D (256,100)\n",
    "    X, T = np.meshgrid(x,t)\n",
    "\n",
    "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "    # Preparing the testing u_star\n",
    "    u_star = Exact_u.flatten()[:,None]\n",
    "                \n",
    "    # Noiseless data TODO: add support for noisy data    \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx,:]\n",
    "    u_train = u_star[idx,:]\n",
    "\n",
    "    if N_0 != None and N_1 != None:\n",
    "      Exact_u = Exact_u.T\n",
    "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
    "      x_0 = x[idx_x,:]\n",
    "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
    "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "          \n",
    "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
    "      x_1 = x[idx_x,:]\n",
    "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
    "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
    "      \n",
    "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
    "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
    "\n",
    "      # Load IRK weights\n",
    "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
    "      IRK_alpha = weights[0:-1,:]\n",
    "      IRK_beta = weights[-1:,:] \n",
    "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
    "\n",
    "    if N_f == None:\n",
    "      lb = X_star.min(axis=0)\n",
    "      ub = X_star.max(axis=0) \n",
    "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "    lb = X_star.min(axis=0)\n",
    "    ub = X_star.max(axis=0) \n",
    "    # Getting the initial conditions (t=0)\n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    uu1 = Exact_u[0:1,:].T\n",
    "    # Getting the lowest boundary conditions (x=-1) \n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    uu2 = Exact_u[:,0:1]\n",
    "    # Getting the highest boundary conditions (x=1) \n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    uu3 = Exact_u[:,-1:]\n",
    "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
    "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "    # Generating the x and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
    "    X_u_train = X_u_train[idx,:]\n",
    "    # Getting the corresponding u_train\n",
    "    u_train = u_train [idx,:]\n",
    "\n",
    "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
    "\n",
    "# define logger class\n",
    "class Logger(object):\n",
    "  def __init__(self, frequency=10):\n",
    "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "    self.start_time = time.time()\n",
    "    self.frequency = frequency\n",
    "\n",
    "  def __get_elapsed(self):\n",
    "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "  def __get_error_u(self):\n",
    "    return self.error_fn()\n",
    "\n",
    "  def set_error_fn(self, error_fn):\n",
    "    self.error_fn = error_fn\n",
    "  \n",
    "  def log_train_start(self, model):\n",
    "    print(\"\\nTraining started\")\n",
    "    print(\"================\")\n",
    "    self.model = model\n",
    "    print(self.model.summary())\n",
    "\n",
    "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "    if epoch % self.frequency == 0:\n",
    "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "  def log_train_opt(self, name):\n",
    "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "    print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "  def log_train_end(self, epoch, custom=\"\"):\n",
    "    print(\"==================\")\n",
    "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "# for plotting\n",
    "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
    "\n",
    "  # Interpolating the results on the whole (x,t) domain.\n",
    "  # griddata(points, values, points at which to interpolate, method)\n",
    "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
    "\n",
    "  # Creating the figures\n",
    "  fig, ax = newfig(1.0, 1.1)\n",
    "  ax.axis('off')\n",
    "\n",
    "  ####### Row 0: u(t,x) ##################    \n",
    "  gs0 = gridspec.GridSpec(1, 2)\n",
    "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "  ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "  divider = make_axes_locatable(ax)\n",
    "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "  fig.colorbar(h, cax=cax)\n",
    "\n",
    "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "  ax.set_xlabel('$t$')\n",
    "  ax.set_ylabel('$x$')\n",
    "  ax.legend(frameon=False, loc = 'best')\n",
    "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "  ####### Row 1: u(t,x) slices ##################    \n",
    "  gs1 = gridspec.GridSpec(1, 3)\n",
    "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 0])\n",
    "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')    \n",
    "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 1])\n",
    "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "  ax = plt.subplot(gs1[0, 2])\n",
    "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "  ax.set_xlabel('$x$')\n",
    "  ax.set_ylabel('$u(t,x)$')\n",
    "  ax.axis('square')\n",
    "  ax.set_xlim([-1.1,1.1])\n",
    "  ax.set_ylim([-1.1,1.1])    \n",
    "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  if file != None:\n",
    "    fig.savefig(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Azv7DZp0M-"
   },
   "source": [
    "## Define custom lbfgs\n",
    "This is a custom optimizer, which we use after the Adams optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OjvMe1Avpvh9",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "  global global_time_list, global_last_time\n",
    "  global_time_list = []\n",
    "  global_last_time = time.perf_counter()\n",
    "  \n",
    "def record_time():\n",
    "  global global_last_time, global_time_list\n",
    "  new_time = time.perf_counter()\n",
    "  global_time_list.append(new_time - global_last_time)\n",
    "  global_last_time = time.perf_counter()\n",
    "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "  \"\"\"Returns last interval records in millis.\"\"\"\n",
    "  global global_last_time, global_time_list\n",
    "  if global_time_list:\n",
    "    return 1000 * global_time_list[-1]\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def dot(a, b):\n",
    "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "  return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "  print(s)\n",
    "\n",
    "final_loss = None\n",
    "times = []\n",
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "  \"\"\"\n",
    "\n",
    "  if config.maxIter == 0:\n",
    "    return\n",
    "\n",
    "  global final_loss, times\n",
    "  \n",
    "  maxIter = config.maxIter\n",
    "  maxEval = config.maxEval or maxIter*1.25\n",
    "  tolFun = config.tolFun or 1e-5\n",
    "  tolX = config.tolX or 1e-19\n",
    "  nCorrection = config.nCorrection or 100\n",
    "  lineSearch = config.lineSearch\n",
    "  lineSearchOpts = config.lineSearchOptions\n",
    "  learningRate = config.learningRate or 1\n",
    "  isverbose = config.verbose or False\n",
    "\n",
    "  # verbose function\n",
    "  if isverbose:\n",
    "    verbose = verbose_func\n",
    "  else:\n",
    "    verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "  f, g = opfunc(x)\n",
    "\n",
    "  f_hist = [f]\n",
    "  currentFuncEval = 1\n",
    "  state.funcEval = state.funcEval + 1\n",
    "  p = g.shape[0]\n",
    "\n",
    "  # check optimality of initial point\n",
    "  tmp1 = tf.abs(g)\n",
    "  if tf.reduce_sum(tmp1) <= tolFun:\n",
    "    verbose(\"optimality condition below tolFun\")\n",
    "    return x, f_hist\n",
    "\n",
    "  # optimize for a max of maxIter iterations\n",
    "  nIter = 0\n",
    "  times = []\n",
    "  while nIter < maxIter:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # keep track of nb of iterations\n",
    "    nIter = nIter + 1\n",
    "    state.nIter = state.nIter + 1\n",
    "\n",
    "    ############################################################\n",
    "    ## compute gradient descent direction\n",
    "    ############################################################\n",
    "    if state.nIter == 1:\n",
    "      d = -g\n",
    "      old_dirs = []\n",
    "      old_stps = []\n",
    "      Hdiag = 1\n",
    "    else:\n",
    "      # do lbfgs update (update memory)\n",
    "      y = g - g_old\n",
    "      s = d*t\n",
    "      ys = dot(y, s)\n",
    "      \n",
    "      if ys > 1e-10:\n",
    "        # updating memory\n",
    "        if len(old_dirs) == nCorrection:\n",
    "          # shift history by one (limited-memory)\n",
    "          del old_dirs[0]\n",
    "          del old_stps[0]\n",
    "\n",
    "        # store new direction/step\n",
    "        old_dirs.append(s)\n",
    "        old_stps.append(y)\n",
    "\n",
    "        # update scale of initial Hessian approximation\n",
    "        Hdiag = ys/dot(y, y)\n",
    "\n",
    "      # compute the approximate (L-BFGS) inverse Hessian \n",
    "      # multiplied by the gradient\n",
    "      k = len(old_dirs)\n",
    "\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      ro = [0]*nCorrection\n",
    "      for i in range(k):\n",
    "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "        \n",
    "\n",
    "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "      # need to be accessed element-by-element, so don't re-type tensor:\n",
    "      al = [0]*nCorrection\n",
    "\n",
    "      q = -g\n",
    "      for i in range(k-1, -1, -1):\n",
    "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "        q = q - al[i]*old_stps[i]\n",
    "\n",
    "      # multiply by initial Hessian\n",
    "      r = q*Hdiag\n",
    "      for i in range(k):\n",
    "        be_i = dot(old_stps[i], r) * ro[i]\n",
    "        r += (al[i]-be_i)*old_dirs[i]\n",
    "        \n",
    "      d = r\n",
    "      # final direction is in r/d (same object)\n",
    "\n",
    "    g_old = g\n",
    "    f_old = f\n",
    "    \n",
    "    ############################################################\n",
    "    ## compute step length\n",
    "    ############################################################\n",
    "    # directional derivative\n",
    "    gtd = dot(g, d)\n",
    "\n",
    "    # check that progress can be made along that direction\n",
    "    if gtd > -tolX:\n",
    "      verbose(\"Can not make progress along direction.\")\n",
    "      break\n",
    "\n",
    "    # reset initial guess for step size\n",
    "    if state.nIter == 1:\n",
    "      tmp1 = tf.abs(g)\n",
    "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "    else:\n",
    "      t = learningRate\n",
    "\n",
    "\n",
    "    # optional line search: user function\n",
    "    lsFuncEval = 0\n",
    "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "      # perform line search, using user function\n",
    "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "      f_hist.append(f)\n",
    "    else:\n",
    "      # no line search, simply move with fixed-step\n",
    "      x += t*d\n",
    "      \n",
    "      if nIter != maxIter:\n",
    "        # re-evaluate function only if not in last iteration\n",
    "        # the reason we do this: in a stochastic setting,\n",
    "        # no use to re-evaluate that function here\n",
    "        f, g = opfunc(x)\n",
    "        lsFuncEval = 1\n",
    "        f_hist.append(f)\n",
    "\n",
    "\n",
    "    # update func eval\n",
    "    currentFuncEval = currentFuncEval + lsFuncEval\n",
    "    state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "    ############################################################\n",
    "    ## check conditions\n",
    "    ############################################################\n",
    "    if nIter == maxIter:\n",
    "      break\n",
    "\n",
    "    if currentFuncEval >= maxEval:\n",
    "      # max nb of function evals\n",
    "      verbose('max nb of function evals')\n",
    "      break\n",
    "\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <=tolFun:\n",
    "      # check optimality\n",
    "      verbose('optimality condition below tolFun')\n",
    "      break\n",
    "    \n",
    "    tmp1 = tf.abs(d*t)\n",
    "    if tf.reduce_sum(tmp1) <= tolX:\n",
    "      # step size below tolX\n",
    "      verbose('step size below tolX')\n",
    "      break\n",
    "\n",
    "    if tf.abs(f-f_old) < tolX:\n",
    "      # function value changing less than tolX\n",
    "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "      break\n",
    "\n",
    "    if do_verbose:\n",
    "      log_fn(nIter, f.numpy(), True)\n",
    "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "      record_time()\n",
    "      times.append(last_time())\n",
    "\n",
    "    if nIter == maxIter - 1:\n",
    "      final_loss = f.numpy()\n",
    "\n",
    "\n",
    "  # save state\n",
    "  state.old_dirs = old_dirs\n",
    "  state.old_stps = old_stps\n",
    "  state.Hdiag = Hdiag\n",
    "  state.g_old = g_old\n",
    "  state.f_old = f_old\n",
    "  state.t = t\n",
    "  state.d = d\n",
    "\n",
    "  return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "  pass\n",
    "\n",
    "class Struct(dummy):\n",
    "  def __getattribute__(self, key):\n",
    "    if key == '__dict__':\n",
    "      return super(dummy, self).__getattribute__('__dict__')\n",
    "    return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOT-E8C4oAJN"
   },
   "source": [
    "# Problem Statement and Classical PINN Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qrt3ECzcLHp"
   },
   "source": [
    "Burger' Equation:\n",
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8CHqrpafela"
   },
   "source": [
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9Ko6L87J2v_"
   },
   "source": [
    "### Hyperparameters for the classical PINN\n",
    "\n",
    "We define some hyperparamters for our classical PINN like number of epochs, collocation points etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jwWhiecUqbAo",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "#layers = [2, 20, 2, 3, 2, 20, 1]\n",
    "\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000 #1000-2000 for good accuracy\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkimJNtepkKi"
   },
   "source": [
    "## PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KVm9UCvvlyY_"
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "    # Descriptive Sequential Keras model [2, 20, …, 20, 1]\n",
    "    self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]: # add dense layers\n",
    "      self.u_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.nu = nu\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Separating the collocation coordinates\n",
    "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "    \n",
    "  # Defining custom loss\n",
    "  def __loss(self, u, u_pred):\n",
    "    f_pred = self.f_model()\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  # The actual PINN\n",
    "  def f_model(self):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(self.x_f)\n",
    "      tape.watch(self.t_f)\n",
    "      # Packing together the inputs\n",
    "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      u_x = tape.gradient(u, self.x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, self.x_f)\n",
    "    u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    nu = self.get_params(numpy=True)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.nu\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.u_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.f_model()\n",
    "    return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FzMd65dpoHo"
   },
   "source": [
    "## Training and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEKkpHvApf46",
    "lines_to_next_cell": 2,
    "outputId": "69e9933f-777a-4c57-edcb-6d27c55b52cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n",
      "Eager execution: True\n",
      "WARNING:tensorflow:From <ipython-input-4-88ac3395fd89>:133: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU-accerelated: False\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,021\n",
      "Trainable params: 3,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf_epoch =      0  elapsed = 00:00  loss = 2.9103e-01  error = 8.8958e-01  \n",
      "tf_epoch =     10  elapsed = 00:03  loss = 2.1053e-01  error = 7.8298e-01  \n",
      "tf_epoch =     20  elapsed = 00:06  loss = 1.8826e-01  error = 6.7527e-01  \n",
      "tf_epoch =     30  elapsed = 00:08  loss = 1.6335e-01  error = 6.6386e-01  \n",
      "tf_epoch =     40  elapsed = 00:11  loss = 1.5234e-01  error = 6.1550e-01  \n",
      "tf_epoch =     50  elapsed = 00:13  loss = 1.5271e-01  error = 5.5059e-01  \n",
      "tf_epoch =     60  elapsed = 00:15  loss = 1.5121e-01  error = 6.1823e-01  \n",
      "tf_epoch =     70  elapsed = 00:17  loss = 1.4983e-01  error = 5.9296e-01  \n",
      "tf_epoch =     80  elapsed = 00:20  loss = 1.4985e-01  error = 6.3547e-01  \n",
      "tf_epoch =     90  elapsed = 00:22  loss = 1.4213e-01  error = 5.9023e-01  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:27  loss = 1.0505e-01  error = 5.0906e-01  \n",
      "nt_epoch =     20  elapsed = 00:29  loss = 8.8589e-02  error = 5.1678e-01  \n",
      "nt_epoch =     30  elapsed = 00:32  loss = 7.3429e-02  error = 4.9310e-01  \n",
      "nt_epoch =     40  elapsed = 00:34  loss = 6.4808e-02  error = 4.7242e-01  \n",
      "nt_epoch =     50  elapsed = 00:38  loss = 6.1181e-02  error = 4.5701e-01  \n",
      "nt_epoch =     60  elapsed = 00:42  loss = 5.7321e-02  error = 4.4796e-01  \n",
      "nt_epoch =     70  elapsed = 00:45  loss = 5.3533e-02  error = 4.3726e-01  \n",
      "nt_epoch =     80  elapsed = 00:48  loss = 4.8882e-02  error = 4.2257e-01  \n",
      "nt_epoch =     90  elapsed = 00:50  loss = 4.5948e-02  error = 4.0895e-01  \n",
      "nt_epoch =    100  elapsed = 00:53  loss = 4.9195e-02  error = 3.6231e-01  \n",
      "nt_epoch =    110  elapsed = 00:55  loss = 3.8547e-02  error = 3.6600e-01  \n",
      "nt_epoch =    120  elapsed = 00:58  loss = 3.6447e-02  error = 3.6048e-01  \n",
      "nt_epoch =    130  elapsed = 01:00  loss = 3.5361e-02  error = 3.4037e-01  \n",
      "nt_epoch =    140  elapsed = 01:03  loss = 2.9490e-02  error = 3.2457e-01  \n",
      "nt_epoch =    150  elapsed = 01:05  loss = 2.6828e-02  error = 3.0163e-01  \n",
      "nt_epoch =    160  elapsed = 01:08  loss = 2.4386e-02  error = 2.8114e-01  \n",
      "nt_epoch =    170  elapsed = 01:10  loss = 2.1817e-02  error = 2.6734e-01  \n",
      "nt_epoch =    180  elapsed = 01:13  loss = 1.9187e-02  error = 2.5328e-01  \n",
      "nt_epoch =    190  elapsed = 01:15  loss = 1.6364e-02  error = 2.3895e-01  \n",
      "nt_epoch =    200  elapsed = 01:17  loss = 1.3976e-02  error = 2.3188e-01  \n",
      "nt_epoch =    210  elapsed = 01:20  loss = 1.2320e-02  error = 2.2311e-01  \n",
      "nt_epoch =    220  elapsed = 01:22  loss = 1.1076e-02  error = 2.2193e-01  \n",
      "nt_epoch =    230  elapsed = 01:26  loss = 1.0296e-02  error = 2.1968e-01  \n",
      "nt_epoch =    240  elapsed = 01:29  loss = 9.8623e-03  error = 2.2067e-01  \n",
      "nt_epoch =    250  elapsed = 01:32  loss = 9.2227e-03  error = 2.1938e-01  \n",
      "nt_epoch =    260  elapsed = 01:34  loss = 8.7110e-03  error = 2.2019e-01  \n",
      "nt_epoch =    270  elapsed = 01:37  loss = 8.0194e-03  error = 2.1298e-01  \n",
      "nt_epoch =    280  elapsed = 01:39  loss = 7.3467e-03  error = 1.9925e-01  \n",
      "nt_epoch =    290  elapsed = 01:42  loss = 6.5754e-03  error = 1.8131e-01  \n",
      "nt_epoch =    300  elapsed = 01:44  loss = 5.6705e-03  error = 1.4213e-01  \n",
      "nt_epoch =    310  elapsed = 01:47  loss = 5.2391e-03  error = 1.2579e-01  \n",
      "nt_epoch =    320  elapsed = 01:49  loss = 4.8328e-03  error = 1.1488e-01  \n",
      "nt_epoch =    330  elapsed = 01:52  loss = 4.5026e-03  error = 1.1320e-01  \n",
      "nt_epoch =    340  elapsed = 01:54  loss = 4.2605e-03  error = 1.1155e-01  \n",
      "nt_epoch =    350  elapsed = 01:57  loss = 4.0567e-03  error = 1.0810e-01  \n",
      "nt_epoch =    360  elapsed = 01:59  loss = 3.6479e-03  error = 1.0327e-01  \n",
      "nt_epoch =    370  elapsed = 02:01  loss = 3.3851e-03  error = 9.8507e-02  \n",
      "nt_epoch =    380  elapsed = 02:04  loss = 3.1936e-03  error = 9.8547e-02  \n",
      "nt_epoch =    390  elapsed = 02:06  loss = 2.9437e-03  error = 9.6901e-02  \n",
      "nt_epoch =    400  elapsed = 02:09  loss = 2.7478e-03  error = 9.6243e-02  \n",
      "nt_epoch =    410  elapsed = 02:11  loss = 2.6370e-03  error = 9.7381e-02  \n",
      "nt_epoch =    420  elapsed = 02:13  loss = 2.4769e-03  error = 9.4409e-02  \n",
      "nt_epoch =    430  elapsed = 02:16  loss = 2.3704e-03  error = 9.3275e-02  \n",
      "nt_epoch =    440  elapsed = 02:18  loss = 2.2759e-03  error = 9.0157e-02  \n",
      "nt_epoch =    450  elapsed = 02:21  loss = 2.1830e-03  error = 8.6636e-02  \n",
      "nt_epoch =    460  elapsed = 02:23  loss = 2.1072e-03  error = 8.5785e-02  \n",
      "nt_epoch =    470  elapsed = 02:26  loss = 2.0084e-03  error = 8.4938e-02  \n",
      "nt_epoch =    480  elapsed = 02:28  loss = 1.9088e-03  error = 8.2869e-02  \n",
      "nt_epoch =    490  elapsed = 02:31  loss = 1.8169e-03  error = 8.1893e-02  \n",
      "nt_epoch =    500  elapsed = 02:34  loss = 1.7691e-03  error = 8.0108e-02  \n",
      "nt_epoch =    510  elapsed = 02:36  loss = 1.7046e-03  error = 7.9794e-02  \n",
      "nt_epoch =    520  elapsed = 02:39  loss = 1.6592e-03  error = 7.7760e-02  \n",
      "nt_epoch =    530  elapsed = 02:41  loss = 1.6362e-03  error = 7.8438e-02  \n",
      "nt_epoch =    540  elapsed = 02:44  loss = 1.6140e-03  error = 7.8116e-02  \n",
      "nt_epoch =    550  elapsed = 02:47  loss = 1.5940e-03  error = 7.8374e-02  \n",
      "nt_epoch =    560  elapsed = 02:49  loss = 1.5564e-03  error = 7.6804e-02  \n",
      "nt_epoch =    570  elapsed = 02:52  loss = 1.5296e-03  error = 7.6366e-02  \n",
      "nt_epoch =    580  elapsed = 02:54  loss = 1.4942e-03  error = 7.6994e-02  \n",
      "nt_epoch =    590  elapsed = 02:57  loss = 1.4640e-03  error = 7.5667e-02  \n",
      "nt_epoch =    600  elapsed = 02:59  loss = 1.4317e-03  error = 7.3030e-02  \n",
      "nt_epoch =    610  elapsed = 03:02  loss = 1.4129e-03  error = 7.2356e-02  \n",
      "nt_epoch =    620  elapsed = 03:04  loss = 1.3799e-03  error = 7.2607e-02  \n",
      "nt_epoch =    630  elapsed = 03:06  loss = 1.3616e-03  error = 7.1954e-02  \n",
      "nt_epoch =    640  elapsed = 03:09  loss = 1.3352e-03  error = 7.0668e-02  \n",
      "nt_epoch =    650  elapsed = 03:12  loss = 1.3072e-03  error = 6.8217e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =    660  elapsed = 03:14  loss = 1.2820e-03  error = 6.6560e-02  \n",
      "nt_epoch =    670  elapsed = 03:17  loss = 1.2255e-03  error = 6.5295e-02  \n",
      "nt_epoch =    680  elapsed = 03:19  loss = 1.2029e-03  error = 6.5277e-02  \n",
      "nt_epoch =    690  elapsed = 03:22  loss = 1.1916e-03  error = 6.4748e-02  \n",
      "nt_epoch =    700  elapsed = 03:25  loss = 1.1726e-03  error = 6.4287e-02  \n",
      "nt_epoch =    710  elapsed = 03:27  loss = 1.1424e-03  error = 6.3921e-02  \n",
      "nt_epoch =    720  elapsed = 03:30  loss = 1.1116e-03  error = 6.2858e-02  \n",
      "nt_epoch =    730  elapsed = 03:34  loss = 1.0953e-03  error = 6.3194e-02  \n",
      "nt_epoch =    740  elapsed = 03:37  loss = 1.0818e-03  error = 6.3267e-02  \n",
      "nt_epoch =    750  elapsed = 03:39  loss = 1.0653e-03  error = 6.3256e-02  \n",
      "nt_epoch =    760  elapsed = 03:42  loss = 1.0525e-03  error = 6.3022e-02  \n",
      "nt_epoch =    770  elapsed = 03:45  loss = 1.0386e-03  error = 6.2725e-02  \n",
      "nt_epoch =    780  elapsed = 03:48  loss = 1.0282e-03  error = 6.2371e-02  \n",
      "nt_epoch =    790  elapsed = 03:50  loss = 1.0195e-03  error = 6.2096e-02  \n",
      "nt_epoch =    800  elapsed = 03:53  loss = 1.0075e-03  error = 6.1501e-02  \n",
      "nt_epoch =    810  elapsed = 03:55  loss = 9.9420e-04  error = 6.1533e-02  \n",
      "nt_epoch =    820  elapsed = 03:58  loss = 9.8521e-04  error = 6.1258e-02  \n",
      "nt_epoch =    830  elapsed = 04:00  loss = 9.7180e-04  error = 6.1110e-02  \n",
      "nt_epoch =    840  elapsed = 04:03  loss = 9.6064e-04  error = 6.0948e-02  \n",
      "nt_epoch =    850  elapsed = 04:06  loss = 9.4671e-04  error = 6.0550e-02  \n",
      "nt_epoch =    860  elapsed = 04:08  loss = 9.2855e-04  error = 5.9426e-02  \n",
      "nt_epoch =    870  elapsed = 04:11  loss = 9.1425e-04  error = 5.9147e-02  \n",
      "nt_epoch =    880  elapsed = 04:13  loss = 9.0666e-04  error = 5.8249e-02  \n",
      "nt_epoch =    890  elapsed = 04:16  loss = 8.9588e-04  error = 5.7658e-02  \n",
      "nt_epoch =    900  elapsed = 04:18  loss = 8.7928e-04  error = 5.7855e-02  \n",
      "nt_epoch =    910  elapsed = 04:21  loss = 8.6792e-04  error = 5.6906e-02  \n",
      "nt_epoch =    920  elapsed = 04:24  loss = 8.5786e-04  error = 5.6240e-02  \n",
      "nt_epoch =    930  elapsed = 04:27  loss = 8.4440e-04  error = 5.7200e-02  \n",
      "nt_epoch =    940  elapsed = 04:29  loss = 8.2811e-04  error = 5.5376e-02  \n",
      "nt_epoch =    950  elapsed = 04:32  loss = 8.1946e-04  error = 5.5039e-02  \n",
      "nt_epoch =    960  elapsed = 04:35  loss = 8.0682e-04  error = 5.4226e-02  \n",
      "nt_epoch =    970  elapsed = 04:37  loss = 7.9221e-04  error = 5.3956e-02  \n",
      "nt_epoch =    980  elapsed = 04:40  loss = 7.7911e-04  error = 5.2619e-02  \n",
      "nt_epoch =    990  elapsed = 04:42  loss = 7.6723e-04  error = 5.1969e-02  \n",
      "nt_epoch =   1000  elapsed = 04:45  loss = 7.5878e-04  error = 5.1619e-02  \n",
      "nt_epoch =   1010  elapsed = 04:47  loss = 7.4775e-04  error = 5.1484e-02  \n",
      "nt_epoch =   1020  elapsed = 04:50  loss = 7.3429e-04  error = 5.0202e-02  \n",
      "nt_epoch =   1030  elapsed = 04:52  loss = 7.2548e-04  error = 4.9996e-02  \n",
      "nt_epoch =   1040  elapsed = 04:55  loss = 7.3094e-04  error = 5.0954e-02  \n",
      "nt_epoch =   1050  elapsed = 04:57  loss = 6.9803e-04  error = 5.0294e-02  \n",
      "nt_epoch =   1060  elapsed = 04:59  loss = 6.8497e-04  error = 4.9555e-02  \n",
      "nt_epoch =   1070  elapsed = 05:02  loss = 6.7048e-04  error = 4.8973e-02  \n",
      "nt_epoch =   1080  elapsed = 05:04  loss = 6.5931e-04  error = 4.7880e-02  \n",
      "nt_epoch =   1090  elapsed = 05:07  loss = 6.4581e-04  error = 4.7825e-02  \n",
      "nt_epoch =   1100  elapsed = 05:09  loss = 6.3251e-04  error = 4.7190e-02  \n",
      "nt_epoch =   1110  elapsed = 05:12  loss = 6.1826e-04  error = 4.6277e-02  \n",
      "nt_epoch =   1120  elapsed = 05:15  loss = 6.0774e-04  error = 4.4878e-02  \n",
      "nt_epoch =   1130  elapsed = 05:17  loss = 5.9528e-04  error = 4.4361e-02  \n",
      "nt_epoch =   1140  elapsed = 05:20  loss = 5.8840e-04  error = 4.3400e-02  \n",
      "nt_epoch =   1150  elapsed = 05:24  loss = 5.8072e-04  error = 4.3616e-02  \n",
      "nt_epoch =   1160  elapsed = 05:27  loss = 5.8427e-04  error = 4.3302e-02  \n",
      "nt_epoch =   1170  elapsed = 05:29  loss = 5.6376e-04  error = 4.3729e-02  \n",
      "nt_epoch =   1180  elapsed = 05:32  loss = 5.5561e-04  error = 4.3467e-02  \n",
      "nt_epoch =   1190  elapsed = 05:35  loss = 5.4935e-04  error = 4.3404e-02  \n",
      "nt_epoch =   1200  elapsed = 05:39  loss = 5.4373e-04  error = 4.2819e-02  \n",
      "nt_epoch =   1210  elapsed = 05:43  loss = 5.3691e-04  error = 4.2564e-02  \n",
      "nt_epoch =   1220  elapsed = 05:46  loss = 5.2995e-04  error = 4.2852e-02  \n",
      "nt_epoch =   1230  elapsed = 05:49  loss = 5.2375e-04  error = 4.2700e-02  \n",
      "nt_epoch =   1240  elapsed = 05:52  loss = 5.1721e-04  error = 4.2173e-02  \n",
      "nt_epoch =   1250  elapsed = 05:55  loss = 5.1312e-04  error = 4.2572e-02  \n",
      "nt_epoch =   1260  elapsed = 05:57  loss = 5.0613e-04  error = 4.2108e-02  \n",
      "nt_epoch =   1270  elapsed = 06:00  loss = 5.0094e-04  error = 4.2085e-02  \n",
      "nt_epoch =   1280  elapsed = 06:02  loss = 4.9756e-04  error = 4.1783e-02  \n",
      "nt_epoch =   1290  elapsed = 06:05  loss = 4.9191e-04  error = 4.1120e-02  \n",
      "nt_epoch =   1300  elapsed = 06:07  loss = 4.8582e-04  error = 3.9816e-02  \n",
      "nt_epoch =   1310  elapsed = 06:10  loss = 4.8196e-04  error = 3.9216e-02  \n",
      "nt_epoch =   1320  elapsed = 06:12  loss = 4.7750e-04  error = 3.9354e-02  \n",
      "nt_epoch =   1330  elapsed = 06:15  loss = 4.7236e-04  error = 3.9014e-02  \n",
      "nt_epoch =   1340  elapsed = 06:17  loss = 4.6647e-04  error = 3.8336e-02  \n",
      "nt_epoch =   1350  elapsed = 06:21  loss = 4.6107e-04  error = 3.7670e-02  \n",
      "nt_epoch =   1360  elapsed = 06:23  loss = 4.5607e-04  error = 3.7381e-02  \n",
      "nt_epoch =   1370  elapsed = 06:26  loss = 4.5269e-04  error = 3.7083e-02  \n",
      "nt_epoch =   1380  elapsed = 06:28  loss = 4.4867e-04  error = 3.6547e-02  \n",
      "nt_epoch =   1390  elapsed = 06:31  loss = 4.4582e-04  error = 3.6392e-02  \n",
      "nt_epoch =   1400  elapsed = 06:33  loss = 4.4009e-04  error = 3.5958e-02  \n",
      "nt_epoch =   1410  elapsed = 06:36  loss = 4.3405e-04  error = 3.6110e-02  \n",
      "nt_epoch =   1420  elapsed = 06:38  loss = 4.3145e-04  error = 3.5766e-02  \n",
      "nt_epoch =   1430  elapsed = 06:41  loss = 4.2692e-04  error = 3.5629e-02  \n",
      "nt_epoch =   1440  elapsed = 06:44  loss = 4.2369e-04  error = 3.5797e-02  \n",
      "nt_epoch =   1450  elapsed = 06:47  loss = 4.2079e-04  error = 3.5499e-02  \n",
      "nt_epoch =   1460  elapsed = 06:50  loss = 4.1883e-04  error = 3.5583e-02  \n",
      "nt_epoch =   1470  elapsed = 06:53  loss = 4.1693e-04  error = 3.6043e-02  \n",
      "nt_epoch =   1480  elapsed = 06:57  loss = 4.1271e-04  error = 3.6351e-02  \n",
      "nt_epoch =   1490  elapsed = 06:59  loss = 4.0901e-04  error = 3.6411e-02  \n",
      "nt_epoch =   1500  elapsed = 07:02  loss = 4.0551e-04  error = 3.6482e-02  \n",
      "nt_epoch =   1510  elapsed = 07:04  loss = 4.0280e-04  error = 3.6591e-02  \n",
      "nt_epoch =   1520  elapsed = 07:07  loss = 4.0068e-04  error = 3.6482e-02  \n",
      "nt_epoch =   1530  elapsed = 07:10  loss = 3.9838e-04  error = 3.6204e-02  \n",
      "nt_epoch =   1540  elapsed = 07:12  loss = 3.9623e-04  error = 3.6047e-02  \n",
      "nt_epoch =   1550  elapsed = 07:15  loss = 3.9453e-04  error = 3.6172e-02  \n",
      "nt_epoch =   1560  elapsed = 07:17  loss = 3.9259e-04  error = 3.6049e-02  \n",
      "nt_epoch =   1570  elapsed = 07:20  loss = 3.8995e-04  error = 3.5780e-02  \n",
      "nt_epoch =   1580  elapsed = 07:22  loss = 3.8718e-04  error = 3.5721e-02  \n",
      "nt_epoch =   1590  elapsed = 07:25  loss = 3.8404e-04  error = 3.5406e-02  \n",
      "nt_epoch =   1600  elapsed = 07:27  loss = 3.8088e-04  error = 3.5670e-02  \n",
      "nt_epoch =   1610  elapsed = 07:30  loss = 3.7890e-04  error = 3.5740e-02  \n",
      "nt_epoch =   1620  elapsed = 07:32  loss = 3.7584e-04  error = 3.5790e-02  \n",
      "nt_epoch =   1630  elapsed = 07:35  loss = 3.7298e-04  error = 3.5704e-02  \n",
      "nt_epoch =   1640  elapsed = 07:38  loss = 3.7069e-04  error = 3.5630e-02  \n",
      "nt_epoch =   1650  elapsed = 07:40  loss = 3.6756e-04  error = 3.5912e-02  \n",
      "nt_epoch =   1660  elapsed = 07:43  loss = 3.6400e-04  error = 3.6045e-02  \n",
      "nt_epoch =   1670  elapsed = 07:45  loss = 3.6154e-04  error = 3.5405e-02  \n",
      "nt_epoch =   1680  elapsed = 07:49  loss = 3.5802e-04  error = 3.5127e-02  \n",
      "nt_epoch =   1690  elapsed = 07:51  loss = 3.5426e-04  error = 3.5140e-02  \n",
      "nt_epoch =   1700  elapsed = 07:54  loss = 3.5000e-04  error = 3.5110e-02  \n",
      "nt_epoch =   1710  elapsed = 07:56  loss = 3.4705e-04  error = 3.5066e-02  \n",
      "nt_epoch =   1720  elapsed = 07:59  loss = 3.4448e-04  error = 3.5155e-02  \n",
      "nt_epoch =   1730  elapsed = 08:01  loss = 3.4245e-04  error = 3.4821e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =   1740  elapsed = 08:04  loss = 3.4121e-04  error = 3.4618e-02  \n",
      "nt_epoch =   1750  elapsed = 08:07  loss = 3.3940e-04  error = 3.4520e-02  \n",
      "nt_epoch =   1760  elapsed = 08:09  loss = 3.3697e-04  error = 3.4584e-02  \n",
      "nt_epoch =   1770  elapsed = 08:12  loss = 3.3475e-04  error = 3.4651e-02  \n",
      "nt_epoch =   1780  elapsed = 08:14  loss = 3.3353e-04  error = 3.4862e-02  \n",
      "nt_epoch =   1790  elapsed = 08:17  loss = 3.3153e-04  error = 3.4928e-02  \n",
      "nt_epoch =   1800  elapsed = 08:20  loss = 3.2764e-04  error = 3.4991e-02  \n",
      "nt_epoch =   1810  elapsed = 08:22  loss = 3.2482e-04  error = 3.4765e-02  \n",
      "nt_epoch =   1820  elapsed = 08:25  loss = 3.2278e-04  error = 3.4689e-02  \n",
      "nt_epoch =   1830  elapsed = 08:27  loss = 3.2135e-04  error = 3.4326e-02  \n",
      "nt_epoch =   1840  elapsed = 08:30  loss = 3.1876e-04  error = 3.3981e-02  \n",
      "nt_epoch =   1850  elapsed = 08:33  loss = 3.1614e-04  error = 3.4396e-02  \n"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
    "def error():\n",
    "  u_pred, _ = pinn.predict(X_star)\n",
    "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "Wf1QXaK5PlUh",
    "lines_to_next_cell": 0,
    "outputId": "4511dc32-8dd6-407a-b0db-0bc47ba1c0d4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.543x264.146 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "  Exact_u, X, T, x, t, file = \"plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOthgTNvZ6YL"
   },
   "source": [
    "## Quantum PINN Solution\n",
    "Now, we want to build a hybrid quantum neural network. It is fairly similar to the classical PINN, but we integrated a quantum layer and did not use a sequential model. For the quantum layer, we first design a circuit in cirq, and implement a new class QPINN, which incorporates this layer. The model is then trained and the result is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN5wIZd4_6-2"
   },
   "source": [
    "### Quantum layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "NHyOxIYC_6-2",
    "outputId": "1b43641b-53a6-4892-bc7b-495868387d5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"2547.48453125\" height=\"150.0\"><line x1=\"34.7588671875\" x2=\"2517.48453125\" y1=\"25.0\" y2=\"25.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"2517.48453125\" y1=\"75.0\" y2=\"75.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"2517.48453125\" y1=\"125.0\" y2=\"125.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"236.9269140625\" x2=\"236.9269140625\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"451.74527343750003\" x2=\"451.74527343750003\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"666.5636328125\" x2=\"666.5636328125\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"821.3819921875\" x2=\"821.3819921875\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"982.50650390625\" x2=\"982.50650390625\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1209.9371679687501\" x2=\"1209.9371679687501\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1437.3678320312501\" x2=\"1437.3678320312501\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1604.7984960937501\" x2=\"1604.7984960937501\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1868.5138281250001\" x2=\"1868.5138281250001\" y1=\"75.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1991.5040625000001\" x2=\"1991.5040625000001\" y1=\"25.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2111.5040625\" x2=\"2111.5040625\" y1=\"75.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2234.494296875\" x2=\"2234.494296875\" y1=\"25.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2354.494296875\" x2=\"2354.494296875\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2477.48453125\" x2=\"2477.48453125\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><rect x=\"10.0\" y=\"5.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(0, 0): </text><rect x=\"10.0\" y=\"55.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(1, 0): </text><rect x=\"10.0\" y=\"105.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">(2, 0): </text><rect x=\"79.517734375\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"79.517734375\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"99.517734375\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"139.517734375\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"159.517734375\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"139.517734375\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"159.517734375\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"236.9269140625\" cy=\"25.0\" r=\"10.0\" /><rect x=\"199.517734375\" y=\"55.0\" width=\"74.81835937500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"236.9269140625\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(i0/pi)</text><rect x=\"294.33609375000003\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"314.33609375000003\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"294.33609375000003\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"314.33609375000003\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"354.33609375000003\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"374.33609375000003\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"451.74527343750003\" cy=\"25.0\" r=\"10.0\" /><rect x=\"414.33609375000003\" y=\"55.0\" width=\"74.81835937500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"451.74527343750003\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(i1/pi)</text><rect x=\"509.15445312500003\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"529.154453125\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"569.154453125\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"589.154453125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"666.5636328125\" cy=\"25.0\" r=\"10.0\" /><rect x=\"629.154453125\" y=\"55.0\" width=\"74.81835937500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"666.5636328125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(i2/pi)</text><rect x=\"723.9728125\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"743.9728125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"821.3819921875\" cy=\"25.0\" r=\"10.0\" /><rect x=\"783.9728125\" y=\"55.0\" width=\"74.81835937500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"821.3819921875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(i3/pi)</text><rect x=\"878.791171875\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"898.791171875\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"878.791171875\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"898.791171875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"982.50650390625\" cy=\"25.0\" r=\"10.0\" /><rect x=\"938.791171875\" y=\"55.0\" width=\"87.43066406250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"982.50650390625\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(-w0/pi)</text><rect x=\"1046.2218359375001\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1066.2218359375001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1046.2218359375001\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1066.2218359375001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1106.2218359375001\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1126.2218359375001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"1209.9371679687501\" cy=\"25.0\" r=\"10.0\" /><rect x=\"1166.2218359375001\" y=\"55.0\" width=\"87.43066406250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1209.9371679687501\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(-w1/pi)</text><rect x=\"1273.6525000000001\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1293.6525000000001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1333.6525000000001\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1353.6525000000001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"1437.3678320312501\" cy=\"25.0\" r=\"10.0\" /><rect x=\"1393.6525000000001\" y=\"55.0\" width=\"87.43066406250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1437.3678320312501\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(-w2/pi)</text><rect x=\"1501.0831640625001\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1521.0831640625001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><circle cx=\"1604.7984960937501\" cy=\"25.0\" r=\"10.0\" /><rect x=\"1561.0831640625001\" y=\"55.0\" width=\"87.43066406250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1604.7984960937501\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">@^(-w3/pi)</text><rect x=\"1668.5138281250001\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1688.5138281250001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"1668.5138281250001\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1688.5138281250001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><rect x=\"1728.5138281250001\" y=\"5.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1748.5138281250001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1728.5138281250001\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1748.5138281250001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1788.5138281250001\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1808.5138281250001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><circle cx=\"1868.5138281250001\" cy=\"75.0\" r=\"10.0\" /><rect x=\"1848.5138281250001\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1868.5138281250001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"1908.5138281250001\" y=\"105.0\" width=\"42.990234375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1930.0089453125001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">T^-1</text><circle cx=\"1991.5040625000001\" cy=\"25.0\" r=\"10.0\" /><rect x=\"1971.5040625000001\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1991.5040625000001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"2031.5040625000001\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2051.5040625\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><circle cx=\"2111.5040625\" cy=\"75.0\" r=\"10.0\" /><rect x=\"2091.5040625\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2111.5040625\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"2151.5040625\" y=\"105.0\" width=\"42.990234375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2172.9991796875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">T^-1</text><circle cx=\"2234.494296875\" cy=\"25.0\" r=\"10.0\" /><rect x=\"2214.494296875\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2234.494296875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"2274.494296875\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2294.494296875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><rect x=\"2274.494296875\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2294.494296875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><rect x=\"2334.494296875\" y=\"105.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2354.494296875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">H</text><circle cx=\"2354.494296875\" cy=\"25.0\" r=\"10.0\" /><rect x=\"2334.494296875\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2354.494296875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text><rect x=\"2394.494296875\" y=\"55.0\" width=\"42.990234375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2415.9894140625\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">T^-1</text><rect x=\"2394.494296875\" y=\"5.0\" width=\"42.990234375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2415.9894140625\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">T</text><circle cx=\"2477.48453125\" cy=\"25.0\" r=\"10.0\" /><rect x=\"2457.48453125\" y=\"55.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2477.48453125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"Arial\">X</text></svg>"
      ],
      "text/plain": [
       "<cirq.contrib.svg.svg.SVGCircuit at 0x7fa838d95990>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# TFQ / Cirq code for quantum layer: 1 node, input-vector-length = 4 (TODO: make 16)\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "from cirq import H, X, cphase, CNOT, Z, T\n",
    "from cirq.circuits import InsertStrategy\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "\n",
    "# adapted from https://github.com/ghellstern/QuantumNN/blob/master/Multi-QBit-Classifier%20TF%20NN-Encoding_Github.ipynb\n",
    "class SplitBackpropQ(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, upstream_symbols, managed_symbols, managed_init_vals,\n",
    "                 operators):\n",
    "        \"\"\"Create a layer that splits backprop between several variables.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            upstream_symbols: Python iterable of symbols to bakcprop\n",
    "                through this layer.\n",
    "            managed_symbols: Python iterable of symbols to backprop\n",
    "                into variables managed by this layer.\n",
    "            managed_init_vals: Python iterable of initial values\n",
    "                for managed_symbols.\n",
    "            operators: Python iterable of operators to use for expectation.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(SplitBackpropQ)\n",
    "        self.all_symbols = upstream_symbols + managed_symbols\n",
    "        self.upstream_symbols = upstream_symbols\n",
    "        self.managed_symbols = managed_symbols\n",
    "        self.managed_init = managed_init_vals\n",
    "        self.ops = operators\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.managed_weights = self.add_weight(\n",
    "            shape=(1, len(self.managed_symbols)),\n",
    "            initializer=tf.constant_initializer(self.managed_init))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs are: circuit tensor, upstream values\n",
    "        upstream_shape = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_weights = tf.tile(self.managed_weights, [upstream_shape, 1])\n",
    "        joined_params = tf.concat([inputs[1], tiled_up_weights], 1)\n",
    "        return tfq.layers.Expectation()(inputs[0],\n",
    "                                        operators=measurement,\n",
    "                                        symbol_names=self.all_symbols,\n",
    "                                        symbol_values=joined_params)\n",
    "\n",
    "\n",
    "# TODO: Normalize weights and inputs to be between [0, pi / 2]\n",
    "# TODO: replace CPhase-Gate with Multi-controlled Phase gate and use 4 qubits, Input-Size 16 instead\n",
    "# 2 regular qubits, 1 ancilla\n",
    "number_qubits = 3\n",
    "number_regular_qubits = number_qubits - 1\n",
    "\n",
    "# specify parameters to set later with NN inputs and weights using Keras + TFQ\n",
    "regular_qubits = [cirq.GridQubit(i, 0) for i in range(number_regular_qubits)]\n",
    "ancilla = cirq.GridQubit(number_regular_qubits, 0)\n",
    "control_params = sympy.symbols('i0, i1, i2, i3')\n",
    "control_params1 = sympy.symbols('w0, w1, w2, w3')\n",
    "\n",
    "# specify cirq circuit\n",
    "qc = cirq.Circuit()\n",
    "size = len(regular_qubits) ** 2\n",
    "# subtract first input from other inputs to save gates\n",
    "#inputs = []\n",
    "#for i in range(1, size):\n",
    "    #inputs.append(control_params[i] - control_params[0])\n",
    "# do the same for weights\n",
    "#weights = []\n",
    "#for i in range(1, size):\n",
    "    #weights.append(control_params1[i] - control_params1[0])\n",
    "# apply Hadamard gate to all regular qubits to create a superposition\n",
    "qc.append(H.on_each(*regular_qubits))\n",
    "# loop over all inputs in inputvector to encode them to the right base states using phase-shifts\n",
    "for index in range(size):\n",
    "    insert_list = []\n",
    "    # index as binary number\n",
    "    binary = '{0:02b}'.format(index)\n",
    "    # get qubit at digit in binary state (positions of qubits : q0, q1, q2, q3) (figuratively, not actually, we are in superposition after all)\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    # this_phase_gate = MCPhaseGate(value, 3, label=\"this_phase_gate\")\n",
    "    # qc.this_phase_gate(0, 1, 2, 3)\n",
    "    # perform controlled phase shift (for more qubits probably possible using ControlledGate() and MatrixGate()\n",
    "    insert_list.append(cphase(control_params[index])(*regular_qubits))\n",
    "    # \"undo\" the NOT-gates to get back to previous states = apply another not\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    qc.append(insert_list, strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# loop over weights\n",
    "for w in range(size):\n",
    "    insert_list = []\n",
    "    # index as binary number\n",
    "    binary = '{0:02b}'.format(w)\n",
    "    # get qubit at digit in binary state (positions of qubits : q0, q1, q2, q3) (figuratively, not actually, we are in superposition after all)\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    # this_phase_gate = MCPhaseGate(value, 3, label=\"this_phase_gate\")\n",
    "    # qc.this_phase_gate(0, 1, 2, 3)\n",
    "    # perform conjugate transpose controlled phase shift\n",
    "    insert_list.append(cphase((-1) * control_params1[w])(*regular_qubits))\n",
    "    # \"undo\" the NOT-gates to get back to previous states = apply another not\n",
    "    for j in range(len(binary)):\n",
    "        if binary[j] == '0':\n",
    "            insert_list.append(X(regular_qubits[j]))\n",
    "    qc.append(insert_list, strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# apply Hadamard gate to all regular qubits\n",
    "qc.append(H.on_each(*regular_qubits), strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# apply X gate to all regular qubits\n",
    "qc.append(X.on_each(*regular_qubits), strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# collect combined state from all regular qubits with ancilla qubit using Toffoli-Gate\n",
    "# Toffoli-Gate does not work in TFQ -> implement decomposition (compare https://en.wikipedia.org/wiki/Toffoli_gate#/media/File:Qcircuit_ToffolifromCNOT.svg)\n",
    "qc.append([H(ancilla), CNOT(regular_qubits[1], ancilla), cirq.inverse(T(ancilla)), CNOT(regular_qubits[0], ancilla), T(ancilla), CNOT(regular_qubits[1], ancilla), cirq.inverse(T(ancilla)), CNOT(regular_qubits[0], ancilla), T(ancilla), T(regular_qubits[1]), H(ancilla), CNOT(regular_qubits[0], regular_qubits[1]), cirq.inverse(T(regular_qubits[1])), T(regular_qubits[0]), CNOT(regular_qubits[0], regular_qubits[1])], strategy=InsertStrategy.NEW_THEN_INLINE)\n",
    "# end circuit\n",
    "\n",
    "# values to initialize the weights (?)\n",
    "np.random.seed(seed=69)\n",
    "int_values = np.random.rand((len(control_params1)))*np.pi\n",
    "\n",
    "measurement = [Z(ancilla)]\n",
    "\n",
    "# draw circuit (has to be at the end of the cell!)\n",
    "SVGCircuit(qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pThtlyBd_6-2"
   },
   "source": [
    "## Quantum PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UTnQGzs2_6-3"
   },
   "outputs": [],
   "source": [
    "# define QPINN class\n",
    "class QPhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu, control_params, control_params1, int_values, measurement):\n",
    "    # Keras model (not sequential though)\n",
    "    # one additional input layer (qinput) is needed for the quantum circuit as input for the quantum layer\n",
    "    self.qinput = tf.keras.Input(shape=(), dtype=tf.dtypes.string, name=\"qinput\")\n",
    "    self.inputs = tf.keras.layers.Input(shape=(layers[0],))\n",
    "    \n",
    "    # define custom layer for lambda layer\n",
    "    def custom_layer(X):\n",
    "        return 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    self.outputs = tf.keras.layers.Lambda(custom_layer, name=\"lambda_layer\")(self.inputs)\n",
    "    \n",
    "    #self.outputs = tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0)(self.inputs)\n",
    "    for width in layers[1:]:\n",
    "        self.outputs = tf.keras.layers.Dense(width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal')(self.outputs)\n",
    "            \n",
    "    #quantum layer\n",
    "    self.outputs = SplitBackpropQ(control_params, control_params1, int_values, measurement)([self.qinput, self.outputs])\n",
    "    #generate model\n",
    "    self.u_model = tf.keras.Model(inputs=[self.qinput, self.inputs], outputs=self.outputs)\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.nu = nu\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Separating the collocation coordinates\n",
    "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "    \n",
    "  # Defining custom loss\n",
    "  def __loss(self, u, u_pred):\n",
    "    f_pred = self.f_model()\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    return var\n",
    "\n",
    "  # The actual PINN\n",
    "  def f_model(self):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(self.x_f)\n",
    "      tape.watch(self.t_f)\n",
    "      # Packing together the inputs\n",
    "      circuit = tfq.convert_to_tensor([qc])\n",
    "      circuits_x_f = tf.concat([circuit for _ in range(self.x_f.shape[0])], axis=0)\n",
    "      X_f = [circuits_x_f, tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)]\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      # Problem: This does not work yet, as TQF does not provide ways to calculate higher-order gradients just yet\n",
    "      # (compare for instance the issue at https://github.com/tensorflow/quantum/issues/431)\n",
    "      # -> TODO: wait for and then build in fix! (or try to find custom solution?)\n",
    "      u_x = tape.gradient(u, self.x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, self.x_f)\n",
    "    u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    nu = self.get_params(numpy=True)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    return self.nu\n",
    "\n",
    "  def get_weights(self):\n",
    "    w = []\n",
    "    for layer in self.u_model.layers[1:]:\n",
    "      weights_biases = layer.get_weights()\n",
    "      weights = weights_biases[0].flatten()\n",
    "      biases = weights_biases[1]\n",
    "      w.extend(weights)\n",
    "      w.extend(biases)\n",
    "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    #X_u = tf.convert_to_tensor(X_u, dtype=self.dtype) -> already do this outside the method now\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "      self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        loss_value = self.__loss(u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "\n",
    "    # We only use the Adams optimizer for starters\n",
    "    \"\"\"lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True,\n",
    "      lambda epoch, loss, is_iter:\n",
    "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\"\"\"\n",
    "\n",
    "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.f_model()\n",
    "    return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VoI7CC_6-3"
   },
   "source": [
    "### Hyperparameters quantum PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HjmLkFA7_6-4"
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1 hidden layer with 4 width, 1-sized output [u]\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 4]\n",
    "\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 1000\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.3,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 500 # fewer epochs so we can test the network in less time. Increase for higher accuracy\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYy43k1X_6-8"
   },
   "source": [
    "### Train model\n",
    "Unfortunately, TensorFlow Quantum has no higher order gradients implemented for circuits yet, as is already pointed out in an issue at github (https://github.com/tensorflow/quantum/issues/431). Once this issue will be resolved, we will be able to actually train our quantum network properly. For now, you should receive the error: LookupError: No gradient defined for operation 'TfqAdjointGradient' (op type: TfqAdjointGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v0GuVLhH_6-9",
    "outputId": "f9b378a7-22ba-4f9b-c823-14a69dab10db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "here2\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_layer (Lambda)           (None, 2)            0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 20)           60          lambda_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 20)           420         dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 20)           420         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 20)           420         dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 20)           420         dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 20)           420         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 20)           420         dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 20)           420         dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "qinput (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 4)            84          dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "split_backprop_q_1 (SplitBackpr (None, 1)            4           qinput[0][0]                     \n",
      "                                                                 dense_44[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,088\n",
      "Trainable params: 3,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    606\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_gradient_function\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2654\u001b[0m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/registry.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m       raise LookupError(\n\u001b[0;32m---> 97\u001b[0;31m           \"%s registry has no entry for: %s\" % (self._name, name))\n\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: TfqAdjointGradient",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-24840f2174a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcircuits_x_u_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcircuit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mX_u_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcircuits_x_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mqpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f25363914ba5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, tf_epochs, nt_config)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;31m# Optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrap_training_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f25363914ba5>\u001b[0m in \u001b[0;36m__grad\u001b[0;34m(self, X, u)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrap_training_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f25363914ba5>\u001b[0m in \u001b[0;36m__loss\u001b[0;34m(self, u, u_pred)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Defining custom loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f25363914ba5>\u001b[0m in \u001b[0;36mf_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;31m# (compare for instance the issue at https://github.com/tensorflow/quantum/issues/431)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;31m# -> TODO: wait for and then build in fix! (or try to find custom solution?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mu_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Getting the other derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mactual_grad_fn\u001b[0;34m(*result_grads)\u001b[0m\n\u001b[1;32m    444\u001b[0m                          \"@custom_gradient grad_fn.\")\n\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0minput_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m       \u001b[0mvariable_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0mflat_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/python/differentiators/differentiator.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 return self._differentiate_ana(programs, symbol_names,\n\u001b[1;32m    132\u001b[0m                                                \u001b[0msymbol_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpauli_sums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                                                forward_pass_vals, grad)\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_pass_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_quantum/python/differentiators/differentiator.py\u001b[0m in \u001b[0;36m_differentiate_ana\u001b[0;34m(self, programs, symbol_names, symbol_values, pauli_sums, forward_pass_vals, grad)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return None, None, self.differentiate_analytic(\n\u001b[1;32m    163\u001b[0m             \u001b[0mprograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             pauli_sums, forward_pass_vals, grad), \\\n\u001b[0m\u001b[1;32m    165\u001b[0m                \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         \u001b[0mpossible_gradient_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m         executing_eagerly)\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m       flat_outputs = forward_function.call(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;34m\"\"\"Builds or retrieves a forward function for this call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     forward_function = self._functions.forward(\n\u001b[0;32m-> 1433\u001b[0;31m         self._inference_args, self._input_tangents)\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tangents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m   1187\u001b[0m       (self._forward, self._forward_graph, self._backward,\n\u001b[1;32m   1188\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forwardprop_output_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_forwardprop_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m            self._forward_and_backward_functions(inference_args, input_tangents))\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_forward_and_backward_functions\u001b[0;34m(self, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m   1387\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m       self._build_functions_for_outputs(\n\u001b[0;32m-> 1389\u001b[0;31m           outputs, inference_args, input_tangents)\n\u001b[0m\u001b[1;32m   1390\u001b[0m     (forward_function, forward_graph,\n\u001b[1;32m   1391\u001b[0m      \u001b[0mbackward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_tangents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_build_functions_for_outputs\u001b[0;34m(self, outputs, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradients_wrt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m       captures_from_forward = [\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    621\u001b[0m               raise LookupError(\n\u001b[1;32m    622\u001b[0m                   \u001b[0;34m\"No gradient defined for operation '%s' (op type: %s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                   (op.name, op.type))\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m           \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnterGradWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation 'TfqAdjointGradient' (op type: TfqAdjointGradient)"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "nu=0.01/np.pi\n",
    "qpinn = QPhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu, control_params, control_params1, int_values, measurement)\n",
    "X_star = tf.convert_to_tensor(X_star, dtype=qpinn.dtype)\n",
    "circuit = tfq.convert_to_tensor([qc])\n",
    "circuits_x_star = tf.concat([circuit for _ in range(X_star.shape[0])], axis=0)\n",
    "print(\"here2\")\n",
    "X_star = [circuits_x_star, X_star]\n",
    "def error():\n",
    "  u_pred, _ = qpinn.predict(X_star)\n",
    "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "circuits_x_u_train = tf.concat([circuit for _ in range(X_u_train.shape[0])], axis=0)\n",
    "X_u_train=[circuits_x_u_train, tf.convert_to_tensor(X_u_train, dtype=qpinn.dtype)]\n",
    "qpinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = qpinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmhC7-Nh_6--"
   },
   "outputs": [],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
    "  Exact_u, X, T, x, t, file = \"qplot.pdf\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "QDynamicsQPINNsPrototype.ipynb",
   "provenance": []
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
